{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae7ac2ba",
   "metadata": {},
   "source": [
    "## MNIST CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebb6c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch libraries \n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# other data libraries\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11419d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1704aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model architecture\n",
    "class MNIST_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_CNN, self).__init__()\n",
    "\n",
    "        # define input, hidden, output\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)  # 1 channel in, 10 out\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)  # 10 channels in, 20 out\n",
    "        self.conv2_dropout = nn.Dropout2d()  # dropout layer is a regualarization layer (randomly deactivates certain network nodes)\n",
    "        self.fcl1 = nn.Linear(320, 50)\n",
    "        self.fcl2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_dropout(self.conv2(x)), 2))\n",
    "\n",
    "        # flatten data\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fcl1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fcl2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def load_data(self):\n",
    "        # # check if data/MNIST/raw folder exists\n",
    "        # directory_exists = os.path.isdir(\"./data/MNIST/raw\")\n",
    "        # print(directory_exists)\n",
    "\n",
    "        # if not directory_exists:\n",
    "        # load the data\n",
    "        train_data = datasets.MNIST(\n",
    "            root=\"data\",\n",
    "            train=True,\n",
    "            transform=ToTensor(),\n",
    "            download=True\n",
    "        )\n",
    "\n",
    "        test_data = datasets.MNIST(\n",
    "            root=\"data\",\n",
    "            train=False,\n",
    "            transform=ToTensor(),\n",
    "            download=True\n",
    "        )\n",
    "\n",
    "        # load the data into a DataLoader, turn into batches and shuffle\n",
    "        loaders = {\n",
    "            \"train\": DataLoader(train_data, batch_size=100, shuffle=True, num_workers=1),\n",
    "            \"test\": DataLoader(test_data, batch_size=100, shuffle=True, num_workers=1)\n",
    "        }\n",
    "\n",
    "        return loaders\n",
    "        # else:\n",
    "        #     return f\"DATA ALREADY EXISTS\"\n",
    "\n",
    "\n",
    "    def train_model(self, training_data, epochs, optimizer, loss_fuc, device):\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "\n",
    "            for batch_i, (data, target) in enumerate(training_data):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                output = self(data)\n",
    "                loss = loss_fuc(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            if batch_i % 20 == 0:\n",
    "                print(f\"Train Epoch: {epoch} [{batch_i * len(data)}/{len(training_data.dataset)} ({100. * batch_i / len(training_data):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "\n",
    "    def test_model(self, testing_data, loss_fuc, device):\n",
    "        self.eval()\n",
    "\n",
    "        test_loss = correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in testing_data:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = self(data)\n",
    "                test_loss += loss_fuc(output, target).item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(testing_data)\n",
    "        print(f\"\\nTest Set: Average Loss: {test_loss:.4f}, Accuracy: {correct}/{len(testing_data)} ({100. * correct / len(testing_data):.0f}%\\n)\")\n",
    "        \n",
    "\n",
    "\n",
    "    def predict_digit(self, img_tensor):\n",
    "        output = self(img_tensor)\n",
    "        prediction = output.argmax(dim=1, keepdim=True).item()\n",
    "\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        confidence = probs.max().item()\n",
    "\n",
    "        print(f\"Prediction: {prediction}\")\n",
    "        return prediction, confidence\n",
    "\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        print(f\"Model Saved to path: {path}\")\n",
    "\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ef4e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model, optimizer model, loss function\n",
    "model = MNIST_CNN().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fuc = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0677cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the data\n",
    "loaders = model.load_data()\n",
    "\n",
    "# train the model, save it\n",
    "epochs = 11\n",
    "model.train_model(loaders[\"train\"], epochs, optimizer, loss_fuc, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69dbb597",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ./model does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# save the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./model/MNIST_CNN_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 105\u001b[0m, in \u001b[0;36mMNIST_CNN.save_model\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[1;32m--> 105\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Saved to path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bride\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bride\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bride\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory ./model does not exist."
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "model.save_model(\"./model/MNIST_CNN_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
